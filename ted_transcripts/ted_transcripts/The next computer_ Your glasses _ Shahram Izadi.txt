 you're listening to TED Talks Daily, where we bring you new ideas and conversations to spark your curiosity every day. I'm your host, Elise Hugh. Do you have 20-20 vision? What about augmented reality vision? Sharam Izzati is leading the development of AR and XR, that's extended reality, at Google. In his talk, he considers the future of mixed reality and how our lives will be transformed by having AI assistants like Google's Gemini in things like wearable glasses and beyond. For our entire lives, we've been living for a computing revolution. With leaps forward in compute, connectivity, mobile technologies, and now AI. For my part, I've dedicated my entire career to augmented reality, fusing the real world with computing experiences. I started this journey about 25 years ago for my PhD. It might sound groundbreaking, the technology was really primitive back then, but the potential felt limitless when we were experimenting in that university lab. What I didn't know at that time was many of the fundamental innovations for my work would come from AI researchers in a different lab. While I was helping computers see the world, these AI researchers were helping computers reason about the world. Since then, large language models and multimodal reasoning have enabled richer language and image understanding. These models are now fast enough for live conversations where the AI can act on your behalf, and most importantly, with your permission. And augmented reality and virtual reality has moved computing from the rectangular screen to the 360 immersive display to now even the world itself becoming the display. We now refer to this broad collection of experiences as extended reality, or XR. Until now, these innovations have occurred separately and in silos. Well, here comes Act Two of the computing revolution. AI and XR are converging, unlocking radical new ways to interact with technology on your terms. Computers will become more lightweight and personal. They will share your vantage point, understand your real-world context, and have a natural interface that's both simple and conversational. Countless people and companies are innovating in this space. We're excited to contribute to this momentum with Android XR. It's an operating system we're building with Samsung that brings XR hardware, together with Gemini, our AI assistant, to augment human intelligence. It can support a broad range of devices, from headsets to glasses, to form factors we haven't even dreamed of yet. Okay. Let me show you where we're heading by inviting my colleague, Nishta, to the stage. Welcome, Nishta. Hi. These ordinary-looking glasses are packed full of technology. A miniaturized camera and microphones give the AI the ability to see and hear the world. Speakers let you listen to the AI and play music or even take calls. And these glasses also have a tiny high-resolution in lens display that's full color that I'm holding in my hand. It's very, very small. These glasses work with your phone, streaming back and forth, allowing the glasses to be very lightweight and access all your phone apps. And if you're wondering, I'm actually wearing the glasses, too. They're actually helping me see all of you in the audience, because they have prescription lenses inside them. But they're also displaying my speaker notes for me as well. And this is the very first time we're showing these glasses in public. So this is going to be a live demo of conceptual hardware, early software. What could go wrong? Nishta, best of luck. No pressure. Amazing. Woo! Now I'm going to launch Gemini. Hello there. Hi, Gemini. Can you start us off with something fun and creative and write a haiku for what I'm seeing right now? Sure. Faces all aglow. Eager minds await the words. Sparks of thought ignite. Some anxious but happy faces as well. Yeah. The AI sees what Nishta sees, hears what she hears, and is responding in real time. But that's just table stakes. Let's actually step it up a notch with something we call memory. For a rolling contextual window, the AI remembers what you see without having to be told what to keep track of. So you guys may have noticed I snuck a peek back at the shelf a moment ago. I wasn't paying attention, but let's see if Gemini was. Hey, did you happen to catch the title of the white book that was on the shelf behind me? The white book is Atomic Habits by James Clear. That is absolutely right. So let's try something harder. I keep losing my hotel key card. Do you know where I last left the card? The hotel key card is to the right of the music record. Great. For someone as forgetful as me, that's a killer app. Okay. This AI is inherently multimodal, so it's using visual data and natural language to digest complex queries in real time while remembering your past context. Nishta, let's see some more. Yeah. Let's bring this book back. So I don't really have time to read it right now, but I wonder if Gemini might be able to help me summarize what's going on. Like, I don't know. What does this diagram mean? It looks complex. This diagram is labeled the habit line and illustrates the process of habit formation. It shows how automaticity increases with repetitions until a habit is formed. Okay. Okay. Now, imagine being able to communicate and understand any language. The AI can shift its output to speak different languages with dialects and accents that feel human and natural. Yeah. Let's go to this sign over here. Can you translate this to English for me? The sign states, private property, no trespassing. Okay. Let's prove this is a live demo. Does anyone in the audience have a suggestion for a different language to translate to? Farsi. Farsi. We tried Farsi. It failed last time, but let's try it again. Do you want to try Farsi? We can try it. Okay. Because I do speak Farsi. It's my mother tongue, so thank you. Gemini, let's just give this a shot. Can you translate this sign to Farsi for us? Great. Awesome. It speaks Farsi. That's amazing. So, as Shira mentioned, you all may have seen translation demos like this before, but what's new now is that in addition to just saying things in a different language, I can also speak to Gemini in another language. I know Hindi, so let's give this a shot. Okay. Can you tell me how these people are looking at? They are now looking at one and one. They are looking at one and one. So, Gemini said you all look focused and excited, and it has a better accent than I do. All right. Now, let's see how the AI can connect the physical world with your digital content and take action. Yeah. Let's get some music going in here. Okay. Gemini, why don't you play us a track from this record? Sure. Sure. Here's Bad Dreams by Teddy Swims. Perfect. In a few seconds, the AI recognized the album, looked up the track list, and took action using the phone apps. Okay, Nishter, it does look like the demo gods are with us, maybe with you more than me. Let's do one last demo. I know you're keen to give. Yes. This is my first time in Vancouver, and I love going on walks. So, why don't you navigate me to a park nearby with views of the ocean? Okay. I am starting navigation to Lighthouse Park, which has magnificent views of the Pacific Ocean. Is there anything else I can assist you with? Honestly, with these directions and a 3D map, I should be all set, and hopefully I won't look like a tourist. Thank you all. Thank you, Nishter. That was awesome. Okay, we've seen glasses. Now let's turn our attention to the other side of the spectrum, headsets. You've seen these types of devices before, but when we first brought AI to a headset, it completely caught me by surprise. For this demo, we're going to use the Project Wuhan headset that Samsung is launching later this year. Compared to glasses, headsets give you an infinite display for getting work done or immersing yourself in a movie or maybe a TED Talk at some point. Let me bring up my colleague Max to show us even more. Hey, Max. Hello. Go for it, Max. Let's do it. You'll notice we start grounded in the real world, and I'm going to control the entire system with my eyes, hands, and voice. But where things get really interesting is when we invite Gemini in as this conversational companion that can come with us anywhere. Hello. Hey, Gemini. Can you bring up my trip planner for me, please? Of course. Opening up your trip planner. Cool. But I left these windows really disorganized. Can you help with that? Of course I can help with that. No clicks. No keyboards. It's just a conversation, and the AI is taking action. Okay. Some more audience participation. Someone shout out a name of a place you want to visit. Melbourne. Cape Town. Cape Town. Let's go to Cape Town. Okay. Sounds fun. Can you please take me to Cape Town? Cape Town? Certainly. Let me help with organizing the windows. Awesome. And can you also take me to Cape Town? I can certainly do that. Let me take you to Cape Town. And we're very organized as we go there. Perfect. The AI is taking Max's requests, figuring out how best to answer it, opening up the Maps app, and from there, he can actually explore anywhere in the world in this 3D view. Okay. This is pretty incredible. Viewing the world from this angle, I can even zoom in to city levels. But what's really interesting is having an AI here who can see what I see. Can you tell me more about the significance of this place? I can indeed provide information about Table Mountain. Table Mountain holds a profound significance, deeply rooted in both its natural splendor and its rich cultural history. It has long been a spiritual and cultural symbol for the indigenous Koi Koi and San people. Okay. Very good, Gemini. Okay, Max, let's bring this closer to home. How about exploring winter sports, seeing that we're in Vancouver? Okay. Let's get some inspiration for Shoram. Can you please show me 360 videos of this? Sure. Here are some 360 degree powder snowboarding videos. Okay. I'm kind of new to snowboarding, so hopefully this video will help me learn. Although this looks already way more intense than I think I'm ready for. What was the name of that trick? That's a backside 540 with a grab. Okay. The AI knows exactly what Max is looking at, even in rich video content, and can answer any question in real time. Max, are you sure this is Whistler? You know, Shoram, I actually have no idea. I've never been here before. But let's find out. What run do you think this is? It's probably one of the ungroomed runs in Summit Bowl, or the top section of West Ridge on Mount Bachelor, as they are close to the summit and face north towards Three Sisters and Broken Top, the prominent mountains visible in the background. Okay. Mount Bachelor, Oregon. Got it. Okay. Let's have a little bit of fun now. Let's have AI narrate this video. Somebody in the audience, shout out a fun narration style, like nature documentary or something like that. Horror movie. Okay. That's going to be, it might be gory, but let's go for it. It's slightly dark, but let's do it. Can you please describe what you're seeing as if you're an overly enthusiastic character in a horror movie? Oh, darling, this is simply marvelous. A desolate mountainscape? How delightfully ominous. Prepare for a chilling dance with the unknown, where every gust of wind whispers tales of icy terror and lurking shadows. Okay, Gemma, that's great. Return to more positive things another day. Okay, yeah. Okay, Max, I know there's one last demo, and you've been apparently practicing your farming skills? Yes, Sharam. I've been excited to get back to my rural roots and also embrace one of my favorite things, which is playing games. But it becomes a whole new next-level experience when you have an AI by your side, both to help keep you company as you play, but also to help you learn how to play these kind of games in the first place. So for example, what do you think I should do next here? It looks like this is early in the game in Stardew Valley. You need to earn money, so try planting some seeds. First, use your hoe to till the soil and then plant your seeds. It looks like you have some parsnips. Since it isn't raining, you'll also need to water them with your watering can. Okay. Seems like a lot, and maybe robots will do this for us one day. But for now, I'm going to have to return back to my farm later. For one last thing, can you please give a playful two-sentence summary of all the ground we covered today? We soared from organizing a digital realm to traversing snowy mountains, and even found time to farm some virtual parsnips. Quite the adventure today. Quite the adventure indeed. Thanks, Gemini. Fantastic. Thank you, Max. We're entering an exciting new phase of the computing revolution. Headsets and glasses are just the beginning. All this points to a single vision of the future, a world where helpful AI will converge with lightweight XR. XR devices will become increasingly more wearable, giving us instant access to information. While AI is going to become more contextually aware, more conversational, more personalized, working with us on our terms and in our language. We're no longer augmenting our reality, but rather augmenting our intelligence. Thank you so much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. And that's it for today's show. TED Talks Daily is part of the TED Audio Collective. This episode was produced and edited by our team, Martha Estefanos, Oliver Friedman, Brian Green, Lucy Little, Alejandra Salazar, and Tonsika Sarmanivon. It was mixed by Christopher Faze-Bogan. Additional support from Emma Taubner and Daniela Ballarezzo. I'm Elise Hu. I'll be back tomorrow with a fresh idea for your feed. Thanks for listening. Transcription by CastingWords That's next time on the TED Radio Hour podcast from NPR. Subscribe or listen to the TED Radio Hour wherever you get your podcasts. Wow. Jasmine. I love you. I love you. I love you. It's loving you. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.