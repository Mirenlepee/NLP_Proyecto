 Want to go deeper on the business topics that matter? This is the So What from BCG, and I'm your host, Georgie Frost. BCG experts explore the most pressing topics on the minds of the world's top business leaders. Listen wherever you get your podcasts. Hey there. If you've ever felt your confidence slip at work, you're not alone. The good news? Confidence isn't a fixed trait. It's a skill. And like any skill, you can build it with the right tools and practice. I'm Anne Morris, CEO and bestselling author. And together with my wife, Frances Fry, a professor at Harvard Business School, we host the TED podcast Fixable. This season, we're zeroing in on confidence. What it really is, how to strengthen it, and how to help others see you as the leader you already are. So if you're ready to show up with more conviction, to get promoted, to lead with clarity, to do the best work of your career, join us on Fixable wherever you get your podcasts. You're listening to TED Talks Daily, where we bring you new ideas to spark your curiosity every day. I'm your host, Elise Hu. Scientists are getting closer and closer to giving the powers of telepathy to humans, thanks to brain-computer interfaces. In his 2024 talk, researcher Chin Tang Lin shows us an EEG headset that can turn silent thought into words with rather remarkable accuracy. Check it out. How often are you frustrated by the time it takes to accurately get things in your mind into a computer? It is even worse for people like me, whose first language is not based on letters. I live and work in Australia, but I am originally from Taiwan. I moved to Sydney eight years ago and now run a university research center there. Most of us use keyboard every day to get things in our mind into the computer. We have to learn to type. The fact that you have to learn to do some things shows how unnatural it is. The finger-dreaming touchscreen has been around for 60 years. It's convenient, but it's also slow. There are other ways to control the computers. Joysticks or gestures. But they are not very useful in capturing the words in your mind. And it is worse. They are critical to communication for human beings. The problem is about to be over because of AI. Today, I will show you how AI can turn the speech in your mind into words on screen. Getting from the brain to the computer efficiently is a real bottleneck for any computer application. It has been my passion for 25 years. Many of you, or most of you, have heard of brain-computer interface, BCI. I have been working on BCI for the direct communication between the brain and machine since 2004. I developed a series of EEG headsets. Let's do this. But they are not new. What is new is an interface that works in a natural way based on how our brain is working naturally. Imagine reading words when someone is thinking, translating the brain signals into words. Today, you will see this in action. And with no imprint, we are using AI to decode the brain signals on the top of your head and identify the biomarkers of speaking. That means that you can send the words in your mind into the computer with wearable technology. It's exciting. I and I believe it will open up the bottleneck of how we engage with computers. We are making exciting progress in decoding EEG to testers. It's natural. We have had very promising results in decoding EEG when someone is speaking aloud. The frontier we are working on now is to decode EEG when the speech is not spoken aloud. The words flow in your mind when you are listening to others or when you are talking to yourself or thinking. We are well on the way to make it a reality. I am going to invite two of my team, Charles and Daniel, to show it to us again. This is the first world premiere for us. We are getting around 50% accuracy in decoding the brain signals into words when someone is speaking silently. Here shows how it will work. We have a collection of words that we have trained our technology with. They are combined into sentences. Charles will select one sentence and Daniel will read the sentence word by word silently and produce the brain signals that will be picked up by our sensors. Our technology will decode the brain signals into words. We pick up the brain signals with sensors and amplify and filter them to reduce the noise and get the right biomarkers. We use AI for the task. We use deep learning to decode the brain signals into the intended words. And then we use the large language model to make the match of the decoded words and make up for the mistakes in EEG decoding. All of this is going on in AI. But for the user, the interaction is natural through thoughts and in natural language. We are very excited about the advances that we are making in understanding words and sentences. Another thing that is very natural to people is looking at something that has their attention. Imagine if you could select an item just by looking at it, not by picking it off the shelf or punching a cord into the fending machine. Two years ago, in a project about hands-free control of robots, we were very excited about the robot control via visual identification of the flickers. We are now beyond that. We need not any flicker. The AI is making it nature. Daniel is going to look at the photos and select an item in his mind. If it is working as issue, you will see the select item pop up on screen. We use photos for this because they are very controllable. To show that this is not all, but just beer into my presentation, chairs will pick up one item for Daniel to select in mind. Please, chairs. It's a car. It's a car. So when Daniel will select, the car is in his mind. Okay. Hamburg. It's incorrect. It's unlucky that the 30% error rates came with us again. Let's invite Charles Daniel to show it again. When Daniel selects an item in his mind, his brain recognizes and identifies the object and triggers his EEGs or technology. decode the triggers. We are working on our way to the technical challenges. We will work on overcoming the interference issue. That's why I asked the phone to be turned off. Different people have different neural signatures, which are important to decoding accuracy. One reason I brought Daniel along here is because he can give off great neural signatures. Thank you. Thank you. He can give off the great neural signatures as far as our technology is concerned. There are still cable here as well. It is not yet very portable. Probably, one biggest barrier to people to use this will be, how do I turn it off? Any one of you will have had the times when you are happy that people you are with don't know what you are really thinking. There are serious privacy and ethics issues that will have to be dealt with. I am very passionate about how important this technology can be. One exciting point is linking the brain-computer interface to the available computers. You already have a computer on your head. The brain will be a natural interface. It is not only about controlling a computer. The natural BCI also provides another way for people to communicate with people. For example, it allows people who are not able to speak to communicate with others or search us when privacy or silence are required. If your idea of nature is a lovely forest, you could wonder how nature this could be. My answer is is nature language. It's the nature thought process that you are using. there are no unnatural imprint in your body. I am challenging you to think about what you regard as natural communication. Turning the speech in your mind into words. There is a standard way to finish up when talking with people you say just think about it. I hope you are as excited as we are for the prospect of a future in which when you just think about something the words in your mind appear on screen. Thank you. That was Chin Ted Lang at TED AI Vienna in 2024. If you're curious about TED's curation, find out more at TED.com slash curation guidelines. And that's it for today. TED Talks Daily is part of the TED Audio Collective. This episode was produced and edited by our team. Martha Estefanos, Oliver Friedman, Brian Green, Autumn Thompson, and Alejandra Salazar. It was mixed by Christopher Fazey-Bogan. Additional support from Emma Taubner and Daniela Balarezzo. I'm Elise Hugh. I'll be back tomorrow with a fresh idea for your feet. Thanks for listening. Scientists at Alphabet's Moonshot Factory tackle big, serious global problems. But their leader likes to show up on rollerblades sometimes dressed as Gandalf. It's also my way of trying to disarm people and remind them humor and silliness are very close to the wellsprings of creativity. How Astro Teller leads scientists to their breakthroughs. That's next time on the TED Radio Hour podcast from NPR. Subscribe or listen to the TED Radio Hour wherever you get your podcasts.